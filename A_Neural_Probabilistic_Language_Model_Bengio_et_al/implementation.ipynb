{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I wanted to test the model without direct connections (W = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that contains most popoular polish surnames\n",
    "words = open('polish_surnames.txt', 'r', encoding='utf-16').read().splitlines()\n",
    "print(len(words))\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings from string elements (chars really) to ints\n",
    "\n",
    "# all possible characters (vocab)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "v = len(chars) + 1 # vocab size\n",
    "\n",
    "# mappings\n",
    "stoi = { ch:i+1 for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "print(f'vocab_size = {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that builds the dataset with apropriate block_size\n",
    "def build_dataset(data, block_size):\n",
    "    X, Y = [], []\n",
    "    for word in data:\n",
    "        context = [0] * block_size\n",
    "        for ch in word + '.':\n",
    "            index = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(index)\n",
    "            context = context[1:] + [index]\n",
    "\n",
    "    X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 3\n",
    "m = 50  # number of dimenstions\n",
    "h = 100 # number of hidden units\n",
    "learning_rate = 0.1\n",
    "n_iter = 100000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_dataset(words, block_size)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "for x,y in zip(X[:10], Y[:10]):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random splits of data - 90% train set, 10% validation set\n",
    "import random # for reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n = int(0.9 * Y.shape[0])\n",
    "X_train, X_val = X[:n], X[n:]\n",
    "Y_train, Y_val = Y[:n], Y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "\n",
    "C = torch.randn((v, m), generator=g)\n",
    "H = torch.randn((m * block_size, h), generator=g)\n",
    "d = torch.randn(h, generator=g)\n",
    "U = torch.randn((h, v), generator=g)\n",
    "b = torch.randn(v, generator=g)\n",
    "\n",
    "parameters = [C, H, d, U, b]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "\n",
    "    # forward pass\n",
    "x = C[X_train[index]].view(-1, m * block_size)\n",
    "hidden_layer = torch.tanh(x @ H + d)\n",
    "logits = hidden_layer @ U + b\n",
    "loss = F.cross_entropy(logits, Y_train[index])\n",
    "Y_train[index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "step_loss = []\n",
    "\n",
    "for iter in range(n_iter):\n",
    "   # getting minibatch\n",
    "    index = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "\n",
    "    # forward pass\n",
    "    x = C[X_train[index]].view(-1, m * block_size)\n",
    "    hidden_layer = torch.tanh(x @ H + d)\n",
    "    logits = hidden_layer @ U + b\n",
    "    loss = F.cross_entropy(logits, Y_train[index])\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # save data\n",
    "    steps.append(iter)\n",
    "    step_loss.append(loss.log10())\n",
    "\n",
    "    learning_rate = 0.1 if iter < 50000 else 0.01\n",
    "            \n",
    "    # update    \n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # print once in a while\n",
    "    if (iter+1) % 1000 == 0:\n",
    "        print(iter+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting average logged loss every 500 data points \n",
    "plt.plot(torch.tensor(step_loss).view(-1, 500).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate losses on the splits\n",
    "def get_loss(split):\n",
    "    if split == 'train':\n",
    "        X_split, Y_split = X_train, Y_train \n",
    "    else:\n",
    "        X_split, Y_split = X_val, Y_val\n",
    "    x = C[X_split].view(-1, m * block_size)          \n",
    "    hidden_layer = torch.tanh(x @ H + d)\n",
    "    logits = hidden_layer @ U + b    \n",
    "    loss = F.cross_entropy(logits, Y_split)\n",
    "    return loss.item()\n",
    "\n",
    "l = get_loss('train')\n",
    "print(f'train_loss = {l}')\n",
    "l = get_loss('val')\n",
    "print(f'val_loss = {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Playing with hyperparameters\n",
    "v = 68 (vocab size)\n",
    "| block_size | m (dims) | h (n_hidden) | learning_rate* | n_iter | batch_size | train_loss | val_loss | total params|\n",
    "|:---:|:---:|:---:|:--------:|:---:|:---:|:--------:|:--------:|:---:|\n",
    "| 3 | 20 | 100 | 0.1 | 100000 | 32 | 2.0255532264709473 | 2.1562182903289795 | 14328 |\n",
    "| **3** | **20** | **100** | **0.1 / 0.01** | **100000** | **32** | **1.9864557981491089** | **2.1179306507110596** | **14328** |\n",
    "| 3 | 50 | 100 | 0.1 | 100000 | 32 | 2.020906686782837 | 2.1610190868377686 | 25368 |\n",
    "| **3** | **50** | **100** | **0.1 / 0.01** | **100000** | **32** | **1.976877212524414** | **2.11557936668396** | **25368** |\n",
    "| 5 | 50 | 100 | 0.1 | 100000 | 32 | 2.0558860301971436 | 2.179558515548706 | 35368 |\n",
    "| 5 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 2.054567575454712 | 2.1728193759918213 | 35368 |\n",
    "| 8 | 50 | 100 | 0.1 | 100000 | 32 | 2.244689702987671 | 2.353611707687378 | 50368 |\n",
    "| 8 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 2.174644947052002 | 2.279350757598877 | 50368 |\n",
    "| 8 | 50 | 100 | 0.1 / 0.01 | 200000 | 32 | 2.297348737716675 | 2.421205759048462 | 50368 |\n",
    "| 8 | 50 | 100 | 0.1 / 0.01 | 200000 | 64 | 2.2630765438079834 | 2.3948867321014404 | 50368 |\n",
    "| 12 | 50 | 100 | 0.1 / 0.01 | 200000 | 64 | 2.387294054031372 | 2.4829165935516357 | 70368 |\n",
    "| 12 | 50 | 200 | 0.1 / 0.01 | 200000 | 64 | 2.36772084236145 | 2.4919257164001465 | 137268 |\n",
    "| 12 | 50 | 200 | 0.1 | 100000 | 64 | 2.267059564590454 | 2.387253522872925 | 137268 | 137268 |\n",
    "| 12 | 50 | 200 | 0.1 / 0.01 | 100000 | 64 | 2.220881938934326 | 2.344449043273926 | 137268 |\n",
    "| 12 | 50 | 400 | 0.1 / 0.01 | 100000 | 64 | 2.182727336883545 | 2.336707830429077 | 271068 |\n",
    "| 12 | 75(>vocab_size) | 400 | 0.1 / 0.01 | 100000 | 64 | 2.1903207302093506 | 2.337827205657959 | 392768 |\n",
    "| 12 | 75(>vocab_size) | 400 | 0.1 / 0.01 / 0.001 | 100000 | 64 | 2.2579424381256104 | 2.4177167415618896 | 392768 |\n",
    "| 12 | 75(>vocab_size) | 600 | 0.1 / 0.01 | 100000 | 64 | 2.1668105125427246 | 2.376784324645996 | 586568 |\n",
    "\n",
    "*equal amounts of iterations per learning rate<br><br>\n",
    "It looks like increasing model size doesn't improve its performance. Nor does increasing training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g =torch.Generator().manual_seed(42)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size                                        \n",
    "    while True:\n",
    "        x = C[torch.tensor([context])].view(-1, m * block_size)                           \n",
    "        hidden_layer = torch.tanh(x @ H + d)       \n",
    "        logits = hidden_layer @ U + b\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        index = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [index]\n",
    "        out.append(index)\n",
    "        if index == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the best model:\n",
    "<ol>\n",
    "    <li>MATDZICZ.</li>\n",
    "    <li>BODZIEMNICA.</li>\n",
    "    <li>CYMY.</li>\n",
    "    <li>KULA.</li>\n",
    "    <li>STEREMSKI.</li>\n",
    "    <li>PATOKICHANDREDER.</li>\n",
    "    <li>GLIK.</li>\n",
    "    <li>MERAK.</li>\n",
    "    <li>PRÓRCAKAJSKI.</li>\n",
    "    <li>ZABRAK.</li>\n",
    "    <li>TACHERSKI.</li>\n",
    "    <li>ROSOWILTARASZCZEK.</li>\n",
    "    <li>MASZK.</li>\n",
    "    <li>DEPOŃSKA.</li>\n",
    "    <li>SLÓMACZKA.</li>\n",
    "    <li>STELIMOWICZER.</li>\n",
    "    <li>KUTA.</li>\n",
    "    <li>KLOWSKI.</li>\n",
    "    <li>KOMADEMBO.</li>\n",
    "    <li>MAGIECH.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the model with direct connections (W != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 12\n",
    "m = 75  # number of dimenstions\n",
    "h = 400 # number of hidden units\n",
    "learning_rate = 0.1\n",
    "n_iter = 100000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_dataset(words, block_size)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "for x,y in zip(X[:10], Y[:10]):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random splits of data - 90% train set, 10% validation set\n",
    "import random # for reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n = int(0.9 * Y.shape[0])\n",
    "X_train, X_val = X[:n], X[n:]\n",
    "Y_train, Y_val = Y[:n], Y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "\n",
    "C = torch.randn((v, m), generator=g)\n",
    "H = torch.randn((m * block_size, h), generator=g)\n",
    "d = torch.randn(h, generator=g)\n",
    "U = torch.randn((h, v), generator=g)\n",
    "b = torch.randn(v, generator=g)\n",
    "W = torch.randn((m * block_size, v), generator=g)\n",
    "\n",
    "parameters = [C, H, d, U, b, W]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "step_loss = []\n",
    "\n",
    "for iter in range(n_iter):\n",
    "   # getting minibatch\n",
    "    index = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "\n",
    "    # forward pass\n",
    "    x = C[X_train[index]].view(-1, m * block_size)\n",
    "    hidden_layer = torch.tanh(x @ H + d)\n",
    "    logits = x @ W + hidden_layer @ U + b\n",
    "    loss = F.cross_entropy(logits, Y_train[index])\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # save data\n",
    "    steps.append(iter)\n",
    "    step_loss.append(loss.log10())\n",
    "\n",
    "    learning_rate = 0.1 if iter < 50000 else 0.01\n",
    "            \n",
    "    # update    \n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # print once in a while\n",
    "    if (iter+1) % 1000 == 0:\n",
    "        print(iter+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting average logged loss every 500 data points \n",
    "plt.plot(torch.tensor(step_loss).view(-1, 500).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate losses on the splits\n",
    "def get_loss(split):\n",
    "    if split == 'train':\n",
    "        X_split, Y_split = X_train, Y_train \n",
    "    else:\n",
    "        X_split, Y_split = X_val, Y_val\n",
    "    x = C[X_split].view(-1, m * block_size)           \n",
    "    hidden_layer = torch.tanh(x @ H + d)\n",
    "    logits = x @ W + hidden_layer @ U + b    \n",
    "    loss = F.cross_entropy(logits, Y_split)\n",
    "    return loss.item()\n",
    "\n",
    "l = get_loss('train')\n",
    "print(f'train_loss = {l}')\n",
    "l = get_loss('val')\n",
    "print(f'val_loss = {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to test the two models with the lowest loss without direct connection.<br><br>\n",
    "v = 68 (vocab size)\n",
    "| block_size | m (dims) | h (n_hidden) | learning_rate* | n_iter | batch_size |  p_train_loss | train_loss | p_val_loss | val_loss | p_total_params | total_params |\n",
    "|:---:|:---:|:---:|:--------:|:---:|:---:|:--------:|:--------:|:--------:|:--------:|:---:|:---:|\n",
    "| 3 | 20 | 100 | 0.1 / 0.01 | 100000 | 32 | 1.986456 | 1.985039 | 2.117931| 2.103702 | 14328 | 18408 |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 1.976877 | 1.952201 | 2.115579| 2.090269 | 25368 | 35568 |\n",
    "\n",
    "*equal amounts of iterations per learning rate<br><br>\n",
    "Adding direct connection improved model's performance. It's interesting that the bigger model got bigger improvement. Let's test this on previous, even bigger models that had higher loss.\n",
    "| block_size | m (dims) | h (n_hidden) | learning_rate* | n_iter | batch_size |  p_train_loss | train_loss | p_val_loss | val_loss | p_total_params | total_params |\n",
    "|:---:|:---:|:---:|:--------:|:---:|:---:|:--------:|:--------:|:--------:|:--------:|:---:|:---:|\n",
    "| 12 | 50 | 400 | 0.1 / 0.01 | 100000 | 64 | 2.182727 | 2.001002 | 2.336708 | 2.154892 | 271068 | 311868 |\n",
    "| 12 | 75 | 400 | 0.1 / 0.01 | 100000 | 64 | 2.190321 | 2.003366 | 2.337827 | 2.171011 | 271068 | 453968 |\n",
    "\n",
    "Direct connection helped a lot more with bigger model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g =torch.Generator().manual_seed(42)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size                                        \n",
    "    while True:\n",
    "        x = C[torch.tensor([context])].view(-1, m * block_size)                            \n",
    "        hidden_layer = torch.tanh(x @ H + d)       \n",
    "        logits = x @ W + hidden_layer @ U + b\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        index = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [index]\n",
    "        out.append(index)\n",
    "        if index == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the best model:\n",
    "<ol>\n",
    "    <li>MAKAVICHOWSKI.</li>\n",
    "    <li>BILIPACHAMO.</li>\n",
    "    <li>KULA.</li>\n",
    "    <li>STARA.</li>\n",
    "    <li>SZYN.</li>\n",
    "    <li>KOCZACHANDRENDROWICKI.</li>\n",
    "    <li>KURPOV.</li>\n",
    "    <li>BRZEWSKI.</li>\n",
    "    <li>TYMARGAŃDOSAJERSKI.</li>\n",
    "    <li>RADOS.</li>\n",
    "    <li>LAASKA.</li>\n",
    "    <li>HAŁĄDŹ.</li>\n",
    "    <li>SZKO.</li>\n",
    "    <li>TABASIA.</li>\n",
    "    <li>SZLADCZKA.</li>\n",
    "    <li>STELINIAK.</li>\n",
    "    <li>PIETKOWSKI.</li>\n",
    "    <li>WOJTOSI.</li>\n",
    "    <li>MADEMBOGUDZIKIEWICZ.</li>\n",
    "    <li>WÓLKA.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I implemented those \"features\" to try and see what's the best result I can get from this architecture.\n",
    "<ol>\n",
    "    <li>Paramters initialization adjustment</li>\n",
    "    <li>Layer Norm / Batch Norm</li>\n",
    "<ol>\n",
    "<br>\n",
    "Model that I tested on:\n",
    "\n",
    "| block_size | m (dims) | h (n_hidden) | learning_rate* | n_iter | batch_size | total_params |\n",
    "|:---:|:---:|:---:|:--------:|:---:|:---:|:---:|\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 35568 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 3\n",
    "m = 50  # number of dimenstions\n",
    "h = 400 # number of hidden units\n",
    "learning_rate = 0.1\n",
    "n_iter = 100000\n",
    "batch_size = 32\n",
    "kaiming_init = 5/3 / (m * block_size)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_dataset(words, block_size)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "for x,y in zip(X[:10], Y[:10]):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random splits of data - 90% train set, 10% validation set\n",
    "import random # for reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n = int(0.9 * Y.shape[0])\n",
    "X_train, X_val = X[:n], X[n:]\n",
    "Y_train, Y_val = Y[:n], Y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "\n",
    "C = torch.randn((v, m), generator=g)\n",
    "H = torch.randn((m * block_size, h), generator=g) * kaiming_init\n",
    "# d = torch.randn(h, generator=g) * 0.01 for normalization\n",
    "U = torch.randn((h, v), generator=g) * 0.01\n",
    "b = torch.randn(v, generator=g) * 0.01\n",
    "W = torch.randn((m * block_size, v), generator=g) * 0.01\n",
    "\n",
    "# batch norm\n",
    "bn_gain = torch.ones((1, h))\n",
    "bn_bias = torch.zeros((1, h))\n",
    "bn_mean_running = torch.zeros((1, h))\n",
    "bn_std_running = torch.ones((1, h))\n",
    "\n",
    "# layer norm\n",
    "ln_gain = torch.ones((1, h))\n",
    "ln_bias = torch.zeros((1, h))\n",
    "\n",
    "parameters = [C, H, U, b, W, ln_gain, ln_bias] # without d for normalization\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_train(batch, bn_gain, bn_bias, bn_mean_running, bn_std_running):\n",
    "    bn_mean_per_i = batch.mean(0, keepdim=True)\n",
    "    bn_std_per_i = batch.std(0, keepdim=True)\n",
    "    batch = bn_gain * (batch - bn_mean_per_i) / bn_std_per_i + bn_bias\n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = 0.999 * bn_mean_running + 0.001 * bn_mean_per_i\n",
    "        bn_std_running = 0.999 * bn_std_running + 0.001 * bn_std_per_i\n",
    "    return batch\n",
    "\n",
    "def batch_norm_test(batch, bn_gain, bn_bias, bn_mean_running, bn_std_running):\n",
    "    batch = bn_gain * (batch - bn_mean_running) / bn_std_running + bn_bias\n",
    "    return batch\n",
    "    \n",
    "def layer_norm(batch, ln_gain, ln_bias):\n",
    "    ln_mean = batch.mean(1, keepdim=True)\n",
    "    ln_std = batch.std(1, keepdim=True)\n",
    "    batch = ln_gain * (batch - ln_mean) / ln_std + ln_bias\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "step_loss = []\n",
    "\n",
    "for iter in range(n_iter):\n",
    "   # getting minibatch\n",
    "    index = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "\n",
    "    # forward pass\n",
    "    x = C[X_train[index]].view(-1, m * block_size)\n",
    "    hidden_layer_pre_act = x @ H #+ d for normalization\n",
    "    hidden_layer_pre_act = layer_norm(hidden_layer_pre_act, ln_gain, ln_bias)\n",
    "    hidden_layer = torch.tanh(hidden_layer_pre_act)\n",
    "    logits = x @ W + hidden_layer @ U + b\n",
    "    loss = F.cross_entropy(logits, Y_train[index])\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # save data\n",
    "    steps.append(iter)\n",
    "    step_loss.append(loss.log10())\n",
    "\n",
    "    learning_rate = 0.1 if iter < 50000 else 0.01\n",
    "            \n",
    "    # update    \n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # print once in a while\n",
    "    if (iter+1) % 1000 == 0:\n",
    "        print(iter+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting average logged loss every 500 data points \n",
    "plt.plot(torch.tensor(step_loss).view(-1, 500).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate losses on the splits\n",
    "def get_loss(split):\n",
    "    if split == 'train':\n",
    "        X_split, Y_split = X_train, Y_train \n",
    "    else:\n",
    "        X_split, Y_split = X_val, Y_val\n",
    "    x = C[X_split].view(-1, m * block_size)          \n",
    "    hidden_layer_pre_act = x @ H #+ d for normalization\n",
    "    hidden_layer_pre_act = layer_norm(hidden_layer_pre_act, ln_gain, ln_bias)\n",
    "    hidden_layer = torch.tanh(hidden_layer_pre_act)\n",
    "    logits = x @ W + hidden_layer @ U + b\n",
    "    loss = F.cross_entropy(logits, Y_split)\n",
    "    return loss.item()\n",
    "\n",
    "l = get_loss('train')\n",
    "print(f'train_loss = {l}')\n",
    "l = get_loss('val')\n",
    "print(f'val_loss = {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "v = 68 (vocab size)\n",
    "| block_size | m (dims) | h (n_hidden) | learning_rate* | n_iter | batch_size | train_loss | val_loss | params | param init | norm\n",
    "|:---:|:---:|:---:|:--------:|:---:|:---:|:--------:|:--------:|:---:|:---:|:---:|\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 1.952201 | 2.090269 | 35568 | none | none |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 1.873789 | 2.007544 | 35568 | present | none |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 2.069724 | 2.205111 | 35668 | present | batch |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 64 | 2.145705 | 2.278057 | 35668 | present | batch |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 128 | 2.257887 | 2.391355 | 35668 | present | batch |\n",
    "| 3 | 50 | 100 | 0.1 / 0.01 | 100000 | 32 | 1.892221 | 2.018405 | 35668 | present | layer |\n",
    "| 3 | 50 | 200 | 0.1 / 0.01 | 100000 | 32 | 1.873022 | 2.004731 | 57668 | present | layer |\n",
    "| 3 | 50 | 400 | 0.1 / 0.01 | 100000 | 32 | 1.863271 | 1.998286 | 101668 | present | layer |\n",
    "\n",
    "*equal amounts of iterations per learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g =torch.Generator().manual_seed(42)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size                                        \n",
    "    while True:\n",
    "        x = C[torch.tensor([context])].view(-1, m * block_size)\n",
    "        hidden_layer_pre_act = x @ H #+ d for normalization\n",
    "        hidden_layer_pre_act = layer_norm(hidden_layer_pre_act, ln_gain, ln_bias)                   \n",
    "        hidden_layer = torch.tanh(hidden_layer_pre_act)       \n",
    "        logits = x @ W + hidden_layer @ U + b\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        index = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [index]\n",
    "        out.append(index)\n",
    "        if index == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the best model:\n",
    "<ol>\n",
    "    <li>MAKAVICHOWSKI.</li>\n",
    "    <li>BILIPACHAMY.</li>\n",
    "    <li>KLEJ.</li>\n",
    "    <li>STARA.</li>\n",
    "    <li>SZYN.</li>\n",
    "    <li>KOCKI.</li>\n",
    "    <li>RADZIEDA.</li>\n",
    "    <li>WIDLAK.</li>\n",
    "    <li>KURTOWSKI.</li>\n",
    "    <li>RYCH.</li>\n",
    "    <li>DYSZ.</li>\n",
    "    <li>BRAŃCZAK.</li>\n",
    "    <li>WASIYK.</li>\n",
    "    <li>LIŃSKI.</li>\n",
    "    <li>ANDERCZEK.</li>\n",
    "    <li>MASZKO.</li>\n",
    "    <li>TABASIA.</li>\n",
    "    <li>SLYMACZKA.</li>\n",
    "    <li>WASZEWICZ.</li>\n",
    "    <li>PER.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
